import time
import datetime
import subprocess
import numpy as np
import joblib
import warnings  
import csv  
import os   
from tensorflow.keras.models import load_model

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏°‡∏≠‡∏á‡∏Å‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Node ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
from DecisionEngineV2 import DecisionEngine
from node_manager import NodeManager

warnings.filterwarnings("ignore", category=UserWarning, module='sklearn')


# 1.  CONFIGURATION (MULTI-VARIABLE VERSION)

# ‡πÉ‡∏™‡πà Path ‡∏Ç‡∏≠‡∏á Model ‡πÅ‡∏•‡∏∞ Scaler ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
MODEL_PATH = 'Best_Multi_Var_Model.keras'
SCALER_INPUTS_PATH = 'Multi-Var_Scaler_Inputs.pkl'
SCALER_TARGET_PATH = 'Multi-Var_Scaler_Target.pkl'

WINDOW_SIZE = 30
LOG_FILE = 'autoscaler_multi_log.csv' 
LOOP_INTERVAL = 60 

NODES = {
    'master': 'aj-aung-k8s-master',
    'worker1': 'aj-aung-k8s-worker1',
    'worker2': 'aj-aung-k8s-worker2'
}
AVAILABLE_WORKERS = ["10.35.29.109", "10.35.29.110"]

# 2.  HELPER FUNCTIONS

def parse_k8s_value(value_str):
    if not value_str: return 0.0
    value_str = str(value_str).strip()
    try:
        if value_str.endswith('m'):
            return float(value_str.replace('m', '')) / 1000.0
        elif value_str.endswith('Ki'):
            return float(value_str.replace('Ki', '')) / (1024.0 * 1024.0) # ‡πÅ‡∏õ‡∏•‡∏á KiB ‡πÄ‡∏õ‡πá‡∏ô GB
        elif value_str.endswith('Mi'):
            return float(value_str.replace('Mi', '')) / 1024.0 # ‡πÅ‡∏õ‡∏•‡∏á MiB ‡πÄ‡∏õ‡πá‡∏ô GB
        elif value_str.endswith('Gi'):
            return float(value_str.replace('Gi', ''))
        return float(value_str)
    except:
        return 0.0

def run_cmd(cmd):
    try: return subprocess.check_output(cmd, shell=True, stderr=subprocess.DEVNULL).decode('utf-8').strip()
    except: return ""

# üåü ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á Memory ‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Worker ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô
def fetch_realtime_data_multivar():
    cluster_cpu_req = 0.0
    cluster_cpu_cap = 0.0
    cluster_mem_req = 0.0
    cluster_mem_cap = 0.0
    
    active_workers = 0
    worker_cpu_usage = 0.0 # ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡∏õ‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡πâ Decision Engine
    
    for key, node in NODES.items():
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Worker (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏≤)
        if key.startswith('worker'):
            status_out = run_cmd(f"kubectl get node {node} --no-headers | awk '{{print $2}}'")
            if "Ready" in status_out and "NotReady" not in status_out:
                active_workers += 1
                
                # 1. ‡∏î‡∏∂‡∏á CPU & Mem Request
                out_req = run_cmd(f"kubectl describe node {node} | grep -A 5 'Allocated resources'")
                lines = out_req.splitlines()
                if len(lines) >= 3:
                    try:
                        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏´‡∏•‡∏±‡∏á header ‡∏Ñ‡∏∑‡∏≠ CPU, ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠ Memory
                        cpu_req_str = lines[1].split()[1]
                        mem_req_str = lines[2].split()[1]
                        cluster_cpu_req += parse_k8s_value(cpu_req_str)
                        cluster_mem_req += parse_k8s_value(mem_req_str)
                    except: pass
                
                # 2. ‡∏î‡∏∂‡∏á CPU & Mem Capacity
                out_cap_cpu = run_cmd(f"kubectl get node {node} -o jsonpath='{{.status.capacity.cpu}}'")
                out_cap_mem = run_cmd(f"kubectl get node {node} -o jsonpath='{{.status.capacity.memory}}'")
                cluster_cpu_cap += parse_k8s_value(out_cap_cpu)
                cluster_mem_cap += parse_k8s_value(out_cap_mem)
                
                # ‡∏î‡∏∂‡∏á CPU Usage ‡∏à‡∏£‡∏¥‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Guardrail)
                out_usage = run_cmd(f"kubectl top node {node} --no-headers | awk '{{print $2}}'")
                worker_cpu_usage += parse_k8s_value(out_usage)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % CPU ‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Worker ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    current_cpu_percent = 0.0
    if cluster_cpu_cap > 0:
        current_cpu_percent = (worker_cpu_usage / cluster_cpu_cap) * 100

    # 3. ‡∏î‡∏∂‡∏á Pending Pods
    pending_count = 0
    try:
        pending_cmd = "kubectl get pods -A --field-selector=status.phase=Pending --no-headers | wc -l"
        pending_count = int(run_cmd(pending_cmd))
    except: pass

    # ‡∏î‡∏∂‡∏á Running Pods (‡πÑ‡∏ß‡πâ‡∏î‡∏π Log)
    running_count = 0
    try:
        running_cmd = "kubectl get pods -A --field-selector=status.phase=Running --no-headers | wc -l"
        running_count = int(run_cmd(running_cmd))
    except: pass

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI + ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Decision Engine
    return (cluster_cpu_req, cluster_cpu_cap, cluster_mem_req, cluster_mem_cap, pending_count), \
           active_workers, current_cpu_percent, running_count

#  2.5 ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LOG

def init_logger():
    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                "Timestamp", "Workers", "CPU_Usage_%", "Running_Pods", "Pending_Pods", 
                "CPU_Req", "CPU_Cap", "Mem_Req", "Mem_Cap",
                "Predicted_CPU_Req", "Action", "Reason"
            ])


# 3.  MAIN SYSTEM LOOP

print("\n" + "="*55)
print(" üß† MULTI-VARIABLE PREDICTIVE AUTOSCALER (PROD) ")
print("="*55)

print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Multi-Var AI ‡πÅ‡∏•‡∏∞ Scalers...")
try:
    model = load_model(MODEL_PATH, compile=False)
    scaler_inputs = joblib.load(SCALER_INPUTS_PATH)
    scaler_target = joblib.load(SCALER_TARGET_PATH)
    
    decision_engine = DecisionEngine(cores_per_node=4.0, max_workers=2, min_workers=1)
    node_bot = NodeManager()
    
    init_logger() 
    print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏∞‡∏ö‡∏ö 5-Dimension AI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f"‚ùå Error ‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
    exit()

history_buffer = [] # ‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö List ‡∏Ç‡∏≠‡∏á 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£
print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Monitor... (‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ {LOOP_INTERVAL} ‡∏ß‡∏¥)\n")

while True:
    try:
        current_dt = datetime.datetime.now()
        timestamp_full = current_dt.strftime("%Y-%m-%d %H:%M:%S")
        timestamp_short = current_dt.strftime("%H:%M:%S")
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡∏°‡∏¥‡∏ï‡∏¥
        ai_features, current_workers, cpu_usage_pct, running_pods = fetch_realtime_data_multivar()
        cpu_req, cpu_cap, mem_req, mem_cap, pending_pods = ai_features
        
        # 2. ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
        history_buffer.append(list(ai_features))
        if len(history_buffer) > WINDOW_SIZE:
            history_buffer.pop(0)

        if len(history_buffer) < WINDOW_SIZE:
            print(f"[{timestamp_short}] ‚è≥ ‡∏™‡∏∞‡∏™‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏´‡πâ AI... ({len(history_buffer)}/{WINDOW_SIZE}) | CPU Req: {cpu_req:.2f}, Mem Req: {mem_req:.2f}GB", end='\r')
            time.sleep(LOOP_INTERVAL) 
            continue

        print(f"\n[{timestamp_short}] " + "‚îÅ"*50)
        print(f"üìä [Status] Workers: {current_workers} | CPU Use: {cpu_usage_pct:.1f}% | Run: {running_pods} | Pend: {pending_pods}")
        print(f"    [Metrics] CPU Req/Cap: {cpu_req:.2f}/{cpu_cap:.2f} | Mem Req/Cap: {mem_req:.2f}/{mem_cap:.2f} GB")

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ AI ‡πÅ‡∏ö‡∏ö Multi-Variable
        raw_array = np.array(history_buffer) # Shape: (30, 5)
        scaled_array = scaler_inputs.transform(raw_array) # Scale ‡∏î‡πâ‡∏ß‡∏¢ scaler_inputs
        X_input = scaled_array.reshape(1, WINDOW_SIZE, 5) # Reshape ‡πÄ‡∏õ‡πá‡∏ô (1, 30, 5)
        
        # 4. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö
        pred_scaled = model.predict(X_input, verbose=0)
        # üö® ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ scaler_target
        predicted_cores = scaler_target.inverse_transform(pred_scaled)[0][0]
        
        print(f"üîÆ [AI Predict] ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï 5 ‡∏ô‡∏≤‡∏ó‡∏µ ‚û°Ô∏è CPU Req ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {predicted_cores:.2f} Cores")

        # 5. ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ DecisionEngine ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢!)
        action, reason = decision_engine.decide(
            predicted_cores=predicted_cores,
            current_workers=current_workers,
            pending_pods=pending_pods,
            current_cpu_usage=cpu_usage_pct,
            current_cpu_req=cpu_req
        )
        
        print(f"ü§ñ [Decision] : {action}")
        print(f"     [Reason] : {reason}")
        
        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log 
        try:
            with open(LOG_FILE, mode='a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    timestamp_full, current_workers, round(cpu_usage_pct, 2), running_pods, pending_pods,
                    round(cpu_req, 2), round(cpu_cap, 2), round(mem_req, 2), round(mem_cap, 2),
                    round(predicted_cores, 2), action, reason
                ])
        except Exception as log_err: pass

        # 7. ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥
        if action == "SCALE_OUT":
            if current_workers < len(AVAILABLE_WORKERS):
                target_ip = AVAILABLE_WORKERS[current_workers]
                print(f"üöÄ [ACTUATOR] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á {target_ip}...")
                node_bot.scale_up(target_ip)
        elif action == "SCALE_IN":
            if current_workers > 0:
                target_ip = AVAILABLE_WORKERS[current_workers - 1]
                print(f"üîª [ACTUATOR] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏¥‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á {target_ip}...")
                node_bot.scale_down(target_ip)

        print("‚îÅ"*56)
        time.sleep(LOOP_INTERVAL)

    except KeyboardInterrupt:
        print("\n\nüõë ‡∏õ‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏ö (Ctrl+C)")
        break
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        time.sleep(5)
